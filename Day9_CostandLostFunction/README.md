# ðŸš€ Day 9: Cost Functions & Loss Functions in Machine Learning

## Overview
On Day 9 of the 100 Days of Machine Learning project, we explored **Cost Functions & Loss Functions**, which play a critical role in optimizing machine learning models. We implemented Mean Squared Error (MSE), Mean Absolute Error (MAE), and Cross-Entropy Loss to gain a deeper understanding of how models learn.

## What We Covered
- **Understanding Cost & Loss Functions**
  - Difference between cost and loss functions
  - Why loss functions are important for optimization
- **Mathematical Formulations**
  - **Mean Squared Error (MSE)**: Measures average squared difference between predicted and actual values
  - **Mean Absolute Error (MAE)**: Measures average absolute difference between predicted and actual values
  - **Cross-Entropy Loss**: Used for classification tasks to measure probability-based errors
- **Dataset Creation & Model Implementation**
  - Generated synthetic datasets for regression and classification
  - Trained a **Linear Regression** model for regression analysis
  - Trained a **Logistic Regression** model for classification tasks
- **Visualization of Loss Functions**
  - 2D plot showing regression fit
  - 3D surface plot to visualize loss landscape

## Results & Observations
- The MSE and MAE values provided insights into regression model performance.
- The cross-entropy loss confirmed the effectiveness of logistic regression in classification.
- The 3D loss surface demonstrated how loss changes with different parameter values.

## Next Steps
- Experiment with different loss functions (Huber Loss, Hinge Loss)
- Implement Gradient Descent on custom cost functions
- Extend to deep learning models using TensorFlow/PyTorch

---
This marks **Day 9** of the **100 Days of ML** journey. Stay tuned for Day 10! ðŸš€ðŸš€ðŸš€

