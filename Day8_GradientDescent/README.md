
"""
# 🚀 Day 8: Gradient Descent
## What is Gradient Descent?
Gradient Descent is an optimization algorithm used to find the minimum of a function. It is commonly used in machine learning to minimize the cost function, helping models learn the best parameters. The idea is simple:

1. Start with random parameter values.
2. Compute the gradient (derivative) of the cost function.
3. Update parameters by moving in the direction that reduces the cost.
4. Repeat until the cost function stops decreasing (convergence).

Mathematically, parameter updates follow this formula:

$$ \theta = \theta - \alpha \cdot \frac{\partial J}{\partial \theta} $$

Where:
- $\theta$ are the model parameters.
- $\alpha$ is the learning rate (step size).
- $\frac{\partial J}{\partial \theta}$ is the gradient of the cost function $J$.
---

## 📝 Tasks for Today

### ✅ Step 1: Start with random parameter values.
### ✅ Step 2: Compute the gradient (derivative) of the cost function.
### ✅ Step 3: Update parameters by moving in the direction that reduces the cost.
### ✅ Step 4: Repeat until the cost function stops decreasing (convergence).
---

## 🏁 Goal
### By the end of the day, you will learn how Gradient Descent works!
---

## 🚀 🚀 Whats's next?
### What are Cost Functions & Loss Functions?

"""
